# config.py
from pathlib import Path

# ============================================================
# ① 路径与数据集选择（只改这里！）
# ============================================================
# 数据所在目录（放你的 swat_clean_*.csv）
DATA_DIR = Path("data")

# 你预处理输出的文件名（按你实际文件名修改）
DATA_FILES = {
    "normal": "swat_clean_normal.csv",
    "merged": "swat_clean_merged.csv",
    "all": "swat_clean_all.csv",
}

# 训练（聚类/建模）使用哪个数据集：只允许 "merged" 或 "all"
TRAIN_DATASET = "normal"   # 可改成 "all"

# 测试/评估使用哪个数据集：一般用 "merged" 或 "all"
TEST_DATASET = "all"

# 标签列名（如果没有这一列，会尝试把“最后一列”当作 label）
LABEL_COL = "label"

# 如果找不到 LABEL_COL，是否把最后一列当 label（你的数据通常是这样）
ASSUME_LAST_COL_AS_LABEL = True

# ============================================================
# ② 通用数据处理（可选调整）
# ============================================================
# 缺失值处理策略（注意：距离/密度类算法不能直接吃 NaN）
# - "ffill_bfill": 先前向填充再后向填充（分块近似）
# - "constant": 常数填充（常用 -1）
# - "none": 不处理（若仍存在 NaN 会报错）
IMPUTE_STRATEGY = "ffill_bfill"
CONSTANT_VALUE = -1.0

# 是否在代码中再次标准化（如果你预处理已经做了 Z-score，一般保持 False）
STANDARDIZE = False

# ============================================================
# ③ 输出设置
# ============================================================
# 每个算法会在各自文件夹下生成 outputs/
SAVE_FULL_RESULTS = True  # 是否保存全量 row_id/score/y_pred
PREVIEW_ROWS = 20000      # 额外保存一份前 N 行预览（便于快速看分布）

# ============================================================
# ④ 可视化开关（开启后生成图表）
# ============================================================
ENABLE_VIS = True

# 可视化采样点数（数据很大时只抽样画图，不然会非常慢）
VIS_SAMPLE_SIZE = 10000
VIS_RANDOM_STATE = 420
VIS_FIG_DPI = 1800

# ============================================================
# ⑤ KMeans 参数
# ============================================================
KMEANS = {
    "n_clusters": 20,
    "batch_size": 8192,
    "random_state": 42,
    "train_chunksize": 200000,
    "test_chunksize": 200000,

    # 阈值：训练集 score 的分位数
    "threshold_quantile": 0.99,
}

# ============================================================
# ⑥ DBSCAN 参数
# ============================================================
DBSCAN = {
    # 保持采样行数
    "train_rows": 100000,

    # 稍微增加 min_samples，过滤掉训练集中的个别噪点
    "min_samples": 50,

    # 【重要】
    # 设置为 -1.0 以启用我们在 run_dbscan.py 中新写的自动估计逻辑 (Mean+3Std)
    # 或者，如果你想手动控制，根据之前的 score 分布，建议直接给 3.5
    "eps": 3.5,

    # 只有当 eps <= 0 时，下面的参数才生效 (现在用上面的 3.5 覆盖)
    "eps_quantile": 0.9999,

    "pca_components": 15,
    "test_chunksize": 100000,
}